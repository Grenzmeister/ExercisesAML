{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sheet01\n",
    "## Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "\n",
    "digits = load_digits()\n",
    "\n",
    "data = digits[\"data\"]\n",
    "images = digits[\"images\"]\n",
    "target = digits[\"target\"]\n",
    "target_names = digits[\"target_names\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract necessary data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X is: (357, 65)\n",
      "Shape of y is: (357,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "for i in range(target.shape[0]):\n",
    "    if target[i]==3:\n",
    "        X.append(data[i])\n",
    "        y.append(1)\n",
    "    elif target[i]==8:\n",
    "        X.append(data[i])\n",
    "        y.append(-1)\n",
    "    else: continue\n",
    "\n",
    "y = np.array(y)\n",
    "X = np.array(X)\n",
    "ones = np.ones(X.shape[0],dtype=float)\n",
    "ones = ones[:, np.newaxis]\n",
    "X = np.concatenate((X, ones),axis=1)\n",
    "print(\"Shape of X is:\", X.shape)\n",
    "print(\"Shape of y is:\", y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression and Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD8CAYAAACb4nSYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAE3xJREFUeJzt3X2MZXV9x/H3dx9GGWsrsBvizrIzGGnrxlIeRqq1CgJNFm1AiGnZbFPamkyiktqmpIHuH01JNsRK+mBKrFNdK90J1FprqakFXaA2qSKzRZAHF1d0F3ZRxrRodWh48Ns/zpnpnbs7e+9l7sydPb/3Kzk593zP79z7PTm7n7lzztx7IjORJJVhzaAbkCStHENfkgpi6EtSQQx9SSqIoS9JBTH0Jakghr4kFcTQl6SCGPqSVJB1g26g3YYNG3JsbGzQbUjSCWXfvn3fy8yNncatutAfGxtjenp60G1I0gklIg52M87TO5JUEENfkgpi6EtSQQx9SSqIoS9JBekY+hGxOyKejoiHFlkfEfGhiDgQEQ9GxLkt666OiG/U09X9bPwYjfQ+rV1bzdetO/Z8w4ZqWrNm4eOxMXjve6v5YstTU9XUXoPe63OWur7Xcb2OXco2/dhWUmeZedwJeCtwLvDQIuvfDnwOCOCNwL11/RTg8Xp+cv345E6vd95552XPYPVN69dnDg0trA0PZ77nPdW82/qePdU+7tmztPVzuh3X69ilbNOPbaXCAdPZIV+z+l/VxSAYO07ofwTY3rK8H3g1sB34yGLjFpsaE/qLTWvX9lYfHa32cXR0aevndDuu17FL2aYf20qF6zb0+3FOfwR4omX5ybq2WP0oETEREdMRMT0zM9OHllaxF1/srX7o0ML5S12/2PLx6r2MXco2/dhWUldWxYXczJzMzPHMHN+4seOniE9sa9f2Vt+yZeH8pa5fbPl49V7GLmWbfmwrqSv9CP3DwOkty5vr2mL1MqxfD0NDC2vDwzAxUc27re/aVT3etWtp6+d0O67XsUvZph/bSupON+eAOP45/Xew8ELuV+r6KcC3qC7inlw/PqXTa72kc/rVCa3epzVrFp5Pb5+femo1RSx8PDpaXXgdHV18ec+eamqvZfZen7PU9b2O63XsUrbpx7ZSwejynH5UYxcXEbcCFwIbgO8CfwSsr39g/FVEBPCXwDZgFvitzJyut/1t4A/rp9qVmR/v9ENofHw8/cI1SepNROzLzPFO4zp+y2Zmbu+wPoH3LbJuN7C702tIklbGqriQK0laGYa+JBXE0Jekghj6klQQQ1+SCmLoS1JBDH1JKoihL0kFMfQlqSDNCf1LLll4V6xLLhl0RxoU774lLaoZoX/JJbB378La3r0Gf4mmpqpvLD14sPpavYMHq2WDXwLo/IVrK+0lfeFaxOLrVtn+aZmNjVVB3250FL797ZXuRlox3X7hWjPe6UtzvPuWdFyGvprFu29Jx9WM0L/44t7qai7vviUdVzNC/wtfODrgL764qqssO3bA5GR1Dj+imk9OVnVJDbmQK0mF80KuJOkohr4kFcTQl6SCGPqSVBBDX5IKYuhLUkEMfUkqiKEvSQUx9CWpIIa+JBWkOaHfetesuUnqh+W8E9dK3eVrUHcTWy13MVstfSxmJfvLzFU1nXfeedmz6lYpx56kpdizJ3N4eOG/qeHhqr6an3sQr7NaXne19rGYPvUHTGcXGduML1zzzllaLst5J66VusvXoO4mtlruYrZa+lhMn/rr9gvXDH3peNasOfa/oQj48Y9X73MP4nVWy+uu1j4W06f+/JZNqR+W805cK3WXr0HdTWy13MVstfSxmBXuz9CXjmc578S1Unf5GtTdxFbLXcxWSx+LWen+ujnxv5LTS7qQW13F8CKulseePZmjo5kR1byfFwCX87kH8Tqr5XVXax+L6UN/FHUhV5IK5zl9SdJRugr9iNgWEfsj4kBEXHeM9aMRsTciHoyIeyJic8u6D0TEQ/X0a/1sXpLUm46hHxFrgZuBS4GtwPaI2No27Cbglsw8C7gBuLHe9h3AucDZwC8A10bET/avfUlSL7p5p38+cCAzH8/M54DbgMvbxmwF7qof392yfivwxcx8ITN/BDwIbFt625Kkl6Kb0B8BnmhZfrKutXoAuLJ+fAXwyog4ta5vi4jhiNgAvA04vf0FImIiIqYjYnpmZqbXfZAkdalfF3KvBS6IiPuBC4DDwIuZeSfwL8B/ALcCXwJebN84Myczczwzxzdu3NinliRJ7boJ/cMsfHe+ua7Ny8wjmXllZp4D7Kxrz9TzXZl5dmb+MhDAY33pXJLUs25C/z7gzIg4IyKGgKuA21sHRMSGiJh7ruuB3XV9bX2ah4g4CzgLuLNfzUuSerOu04DMfCEirgHuANYCuzPz4Yi4geoTYLcDFwI3RkQCXwTeV2++Hvj3qL4Q7QfAr2fmC/3fDUlSN/xEriQ1QHmfyB0eXnjXrPYvMJIkNST0h4fh2WcX1p591uCXpDbNCP32wO9Ul6RCNSP0JUldMfQlqSDNCP2TTuqtLkmFakboz84eHfAnnVTVJUnzOn4464RhwEtSR814py9J6oqhL0kFMfQlqSCGviQVxNCXpIIY+pJUEENfkgpi6EtSQQx9SSqIoS9JBTH0Jakghr4kFcTQl6SCGPqSVBBDX5IKYuhLUkEMfUkqiKEvSQUx9CWpIIa+JBXE0Jekghj6klQQQ1+SCmLoS1JBDH1JKoihL0kFMfQlqSBdhX5EbIuI/RFxICKuO8b60YjYGxEPRsQ9EbG5Zd2fRMTDEfFoRHwoIqKfOyBJ6l7H0I+ItcDNwKXAVmB7RGxtG3YTcEtmngXcANxYb/uLwJuBs4DXA28ALuhb962mpmBsDNasqeZTU8vyMpJ0Iuvmnf75wIHMfDwznwNuAy5vG7MVuKt+fHfL+gReDgwBLwPWA99datNHmZqCiQk4eBAyq/nEhMEvSW26Cf0R4ImW5SfrWqsHgCvrx1cAr4yIUzPzS1Q/BJ6qpzsy89GltXwMO3fC7OzC2uxsVZckzevXhdxrgQsi4n6q0zeHgRcj4rXA64DNVD8oLoqIt7RvHBETETEdEdMzMzO9v/qhQ73VJalQ3YT+YeD0luXNdW1eZh7JzCsz8xxgZ117hupd/5cz84eZ+UPgc8Cb2l8gMyczczwzxzdu3Nj7XmzZ0ltdkgrVTejfB5wZEWdExBBwFXB764CI2BARc891PbC7fnyI6jeAdRGxnuq3gP6f3tm1C4aHF9aGh6u6JGlex9DPzBeAa4A7qAL7k5n5cETcEBGX1cMuBPZHxGPAacBc2n4K+CbwNarz/g9k5j/3dxeAHTtgchJGRyGimk9OVnVJ0rzIzEH3sMD4+HhOT08Pug1JOqFExL7MHO80zk/kSlJBDH1JKoihL0kFMfQlqSCGviQVxNCXpIIY+pJUEENfkgpi6EtSQQx9SSpIc0J/aKj63p25aWho0B1J0qrTjNAfGoLnn19Ye/55g1+S2jQj9NsDv1NdkgrVjNCXJHXF0JekgjQj9Nev760uSYVqRug/99zRAb9+fVWXJM1bN+gG+saAl6SOmvFOX5LUFUNfkgpi6EtSQQx9SSqIoS9JBTH0Jakghr4kFcTQl6SCGPqSVBBDX5IK0pzQn5qCsTFYs6aaT00NuiNJWnWa8d07U1MwMQGzs9XywYPVMsCOHYPrS5JWmWa809+58/8Df87sbFWXJM1rRugfOtRbXZIK1YzQ37Klt7okFaoZob9rFwwPL6wND1d1SdK8ZoT+jh0wOQmjoxBRzScnvYgrSW2a8dc7UAW8IS9Jx9XVO/2I2BYR+yPiQERcd4z1oxGxNyIejIh7ImJzXX9bRHy1ZfrfiHhnv3dCktSdjqEfEWuBm4FLga3A9ojY2jbsJuCWzDwLuAG4ESAz787MszPzbOAiYBa4s4/9S5J60M07/fOBA5n5eGY+B9wGXN42ZitwV/347mOsB3gX8LnMnD3GOknSCugm9EeAJ1qWn6xrrR4ArqwfXwG8MiJObRtzFXDrS2lSktQf/frrnWuBCyLifuAC4DDw4tzKiHg18HPAHcfaOCImImI6IqZnZmb61JIkqV03oX8YOL1leXNdm5eZRzLzysw8B9hZ155pGfKrwD9m5vPHeoHMnMzM8cwc37hxY087IEnqXjehfx9wZkScERFDVKdpbm8dEBEbImLuua4Hdrc9x3Y8tSNJA9cx9DPzBeAaqlMzjwKfzMyHI+KGiLisHnYhsD8iHgNOA+Y/ChsRY1S/KfxbXzuXJPUsMnPQPSwwPj6e09PTg25Dkk4oEbEvM8c7jWvG1zBIkrrSnNAfGam+d2duGmn/q1JJUjNCf2QEjhxZWDtyxOCXpDbNCP32wO9Ul6RCNSP0JUldMfQlqSDNCP1Nm3qrS1KhmhH6hw8fHfCbNlV1SdK85tw5y4CXpI6a8U5fktQVQ1+SCmLoS1JBDH1JKoihL0kFMfQlqSCGviQVxNCXpIIY+pJUEENfkgpi6EtSQQx9SSqIoS9JBTH0Jakghr4kFcTQl6SCGPqSVBBDX5IKYuhLUkEMfUkqiKEvSQUx9CWpIIa+JBXE0Jekghj6klQQQ1+SCmLoS1JBugr9iNgWEfsj4kBEXHeM9aMRsTciHoyIeyJic8u6LRFxZ0Q8GhGPRMRY/9qXJPWiY+hHxFrgZuBSYCuwPSK2tg27CbglM88CbgBubFl3C/DBzHwdcD7wdD8aP8rUFIyNwZo11XxqalleRpJOZN280z8fOJCZj2fmc8BtwOVtY7YCd9WP755bX/9wWJeZnwfIzB9m5mxfOm81NQUTE3DwIGRW84kJg1+S2nQT+iPAEy3LT9a1Vg8AV9aPrwBeGRGnAj8NPBMRn46I+yPig/VvDv21cyfMtv0smZ2t6pKkef26kHstcEFE3A9cABwGXgTWAW+p178BeA3wm+0bR8RERExHxPTMzEzvr37oUG91SSpUN6F/GDi9ZXlzXZuXmUcy88rMPAfYWdeeofqt4Kv1qaEXgM8A57a/QGZOZuZ4Zo5v3Lix973YsqW3uiQVqpvQvw84MyLOiIgh4Crg9tYBEbEhIuae63pgd8u2r4qIuSS/CHhk6W232bULhocX1oaHq7okaV7H0K/foV8D3AE8CnwyMx+OiBsi4rJ62IXA/oh4DDgN2FVv+yLVqZ29EfE1IIC/7vte7NgBk5MwOgoR1XxysqpLkuZFZg66hwXGx8dzenp60G1I0gklIvZl5nincX4iV5IKYuhLUkEMfUkqiKEvSQUx9CWpIIa+JBXE0Jekghj6klQQQ1+SCmLoS1JBmhP6IyPV9+7MTSPtX/kvSWpG6I+MwJEjC2tHjhj8ktSmGaHfHvid6pJUqGaEviSpK4a+JBWkGaG/aVNvdUkqVDNC//DhowN+06aqLkmat27QDfSNAS9JHTXjnb4kqSuGviQVxNCXpIIY+pJUEENfkgoSmTnoHhaIiBng4BKeYgPwvT61c6IobZ9L219wn0uxlH0ezcyNnQatutBfqoiYzszxQfexkkrb59L2F9znUqzEPnt6R5IKYuhLUkGaGPqTg25gAErb59L2F9znUiz7PjfunL4kaXFNfKcvSVpEY0I/IrZFxP6IOBAR1w26n+UQEadHxN0R8UhEPBwR76/rp0TE5yPiG/X85EH32m8RsTYi7o+Iz9bLZ0TEvfXx/ruIGBp0j/0UEa+KiE9FxNcj4tGIeFPTj3NE/F797/qhiLg1Il7etOMcEbsj4umIeKildszjGpUP1fv+YESc248eGhH6EbEWuBm4FNgKbI+IrYPtalm8APx+Zm4F3gi8r97P64C9mXkmsLdebpr3A4+2LH8A+LPMfC3w38C7B9LV8vkL4F8z82eBn6fa98Ye54gYAX4HGM/M1wNrgato3nH+G2BbW22x43opcGY9TQAf7kcDjQh94HzgQGY+npnPAbcBlw+4p77LzKcy8z/rx/9DFQQjVPv6iXrYJ4B3DqbD5RERm4F3AB+tlwO4CPhUPaRR+xwRPwW8FfgYQGY+l5nP0PDjTPVV7ydFxDpgGHiKhh3nzPwi8F9t5cWO6+XALVn5MvCqiHj1UntoSuiPAE+0LD9Z1xorIsaAc4B7gdMy86l61XeA0wbU1nL5c+APgB/Xy6cCz2TmC/Vy0473GcAM8PH6lNZHI+IVNPg4Z+Zh4CbgEFXYfx/YR7OP85zFjuuy5FpTQr8oEfETwD8Av5uZP2hdl9WfYzXmT7Ii4leApzNz36B7WUHrgHOBD2fmOcCPaDuV08DjfDLVO9szgE3AKzj6NEjjrcRxbUroHwZOb1neXNcaJyLWUwX+VGZ+ui5/d+7Xvnr+9KD6WwZvBi6LiG9Tnba7iOp896vq0wDQvOP9JPBkZt5bL3+K6odAk4/zJcC3MnMmM58HPk117Jt8nOcsdlyXJdeaEvr3AWfWV/qHqC4A3T7gnvquPpf9MeDRzPzTllW3A1fXj68G/mmle1sumXl9Zm7OzDGq43pXZu4A7gbeVQ9r2j5/B3giIn6mLl0MPEKDjzPVaZ03RsRw/e98bp8be5xbLHZcbwd+o/4rnjcC3285DfTSZWYjJuDtwGPAN4Gdg+5nmfbxl6h+9XsQ+Go9vZ3qHPde4BvAF4BTBt3rMu3/hcBn68evAb4CHAD+HnjZoPvr876eDUzXx/ozwMlNP87AHwNfBx4C/hZ4WdOOM3Ar1TWL56l+o3v3YscVCKq/Svwm8DWqv2xacg9+IleSCtKU0zuSpC4Y+pJUEENfkgpi6EtSQQx9SSqIoS9JBTH0Jakghr4kFeT/AIGLCQLKMJDTAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The maximum score 1.0 is at lamb = 14.174741629268047\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.7, random_state=0)\n",
    "\n",
    "scores = []\n",
    "C_list = np.geomspace(1e-4,1e2,100)\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "for lamb_inv in C_list:\n",
    "    clf1 = LogisticRegression(C = lamb_inv, fit_intercept = False, random_state = 0, solver = \"liblinear\").fit(X_train,y_train)\n",
    "    plt.hold = True\n",
    "    ax.plot(lamb_inv, clf1.score(X_test,y_test),\"ro\")\n",
    "    scores.append(clf1.score(X_test,y_test))\n",
    "\n",
    "scores = np.array(scores)\n",
    "lamb = 1/C_list[np.argmax(scores)]\n",
    "plt.show()\n",
    "print(\"The maximum score\",np.max(scores),\"is at lamb =\",lamb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Optimization Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    sigma =  1/(1+np.exp(-z))\n",
    "    return sigma\n",
    "\n",
    "\n",
    "def gradient(beta, X, y):\n",
    "    update = np.zeros(beta.shape[0])\n",
    "    for i in range(X.shape[0]):\n",
    "        update = update + sigmoid(y[i]*np.dot(X[i,:],beta)) * y[i] * X[i,:]\n",
    "    grad = (1/reg_param) * beta + (1/y.shape[0]) * update\n",
    "    return grad\n",
    "\n",
    "def gradient_achita(beta, X, y):\n",
    "    N = y.shape[0]\n",
    "    temp = sigmoid(-y*np.dot(X,beta))*y\n",
    "    #Here is the correction of the grad\n",
    "    result = beta - 1.0/N*(np.dot(X.T,temp))\n",
    "    return result\n",
    "\n",
    "def predict(beta,X):\n",
    "    decision = np.dot(X,beta)\n",
    "    y_computed = np.sign(decision)\n",
    "    return y_computed\n",
    "\n",
    "def zero_ones_loss(y_prediction, y_truth):\n",
    "    zero_ones_vec = 1/2 * (y_prediction - y_truth)\n",
    "    return np.dot(zero_ones_vec , zero_ones_vec)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(m,beta,X,y,tau):\n",
    "    for i in range(m):\n",
    "        beta = beta - tau * gradient(beta,X,y)\n",
    "    return beta\n",
    "\n",
    "def stochastic_gradient(m,beta,X,y,tau0,gamma):\n",
    "    tau = tau0\n",
    "    for i in range(m):\n",
    "        tau = tau/(1+np.power(gamma,i))\n",
    "        X_random = X[np.random.randint(0,X.shape[0]),:].reshape(1,65)\n",
    "        beta = beta - tau * gradient(beta, X_random, y)\n",
    "    return beta\n",
    "\n",
    "def SG_mini_batch(m,beta,X,y,tau0,gamma,B):\n",
    "    tau = tau0\n",
    "    for i in range(m):\n",
    "        tau = tau/(1+np.power(gamma,i))\n",
    "        rows_random = np.random.choice(np.arange(X.shape[0]),B,replace = True)\n",
    "        X_random = X[rows_random,:]\n",
    "        beta = beta - tau * gradient(beta,X_random,y)\n",
    "    return beta\n",
    "\n",
    "def SG_momentum(m,beta,X,y,tau0,gamma,mu):\n",
    "    g = np.zeros(65)\n",
    "    tau = tau0\n",
    "    for i in range(m):\n",
    "        tau = tau/(1+np.power(gamma,i))\n",
    "        X_random = X[np.random.randint(0,X.shape[0]),:].reshape(1,65)\n",
    "        g = mu*g +(1-mu)*gradient(beta,X_random,y)\n",
    "        beta = beta - tau*g\n",
    "    return beta\n",
    "\n",
    "def ADAM(m,beta,X,y,tau):\n",
    "    g = 0\n",
    "    q = 1\n",
    "    mu1 = 0.9\n",
    "    mu2 = 0.999\n",
    "    epsilon = 1e-8\n",
    "    for i in range(m):\n",
    "        X_random = X[np.random.randint(0,X.shape[0]),:].reshape(1,65)\n",
    "        grad = gradient(beta,X_random,y)     \n",
    "        g = mu1*g + (1-mu1)*grad\n",
    "        q = mu2*q + (1-mu2)*(grad*grad)\n",
    "        g_tilt = g/(1-np.power(mu1,i+1))\n",
    "        q_tilt = q/(1-np.power(mu2,i+1))  \n",
    "        beta = beta - tau/(np.sqrt(q_tilt)+epsilon) * g_tilt\n",
    "    return beta\n",
    "\n",
    "def SAG(m,beta,X,y,tau0,gamma):\n",
    "    g_stored = []\n",
    "    tau = tau0\n",
    "    for i in range(y.shape[0]):\n",
    "        g_stored.append( y[i]* X[i] * sigmoid(-y[i] * np.dot( X[i] , beta)))\n",
    "    g_stored = np.array(g_stored)\n",
    "    g = np.sum(g_stored,axis = 0)/y.shape[0]\n",
    "    for j in range(m):\n",
    "        tau = tau/(1+np.power(gamma,j))\n",
    "        i = np.random.randint(0,g_stored.shape[0])\n",
    "        g_update = y[i] * np.dot( X[i] , sigmoid(-y[i]* np.dot(X[i] , beta)))\n",
    "        g = g + 1/X.shape[0] *(g_update - g_stored[i])\n",
    "        g_stored[i] = g_update\n",
    "        beta = beta * (1- tau/reg_param) - tau*g \n",
    "    return beta\n",
    "\n",
    "\n",
    "def DCA(m,X,y):\n",
    "    alpha = np.random.rand(y.shape[0])    \n",
    "    beta = reg_param/y.shape[0] * np.dot( alpha * y, X)\n",
    "    for j in range(m):\n",
    "        i = np.random.randint(alpha.shape[0])\n",
    "        f1 = y[i]*np.dot(X[i],beta) + np.log(alpha[i]/(1-alpha[i]))      \n",
    "        f2 = reg_param/y.shape[0] * np.dot(X[i],X[i]) + 1/(alpha[i]*(1-alpha[i]))       \n",
    "        alpha_new = alpha[i] - f1/f2\n",
    "        beta = beta + (reg_param/y.shape[0]) *y[i]* X[i] * (alpha_new - alpha[i])\n",
    "    return beta\n",
    "\n",
    "\n",
    "def newton_raphson(m,beta,X,y):\n",
    "    for t in range(m):\n",
    "        D = X.shape[1]\n",
    "        N = X.shape[0]\n",
    "        beta = np.zeros(D)\n",
    "        z = np.dot( X , beta)\n",
    "        y_tilt = y/sigmoid(np.dot(y,z))\n",
    "        diag = np.array([reg_param/N*sigmoid(z[i])*sigmoid(-z[i]) for i in range(N)])\n",
    "        W = np.diagflat(diag)\n",
    "        beta = np.dot( np.dot( np.linalg.inv( np.identity(D) + np.dot ( np.dot(X.T , W) , X )) , np.dot(X.T,W) ), z + y_tilt)\n",
    "    return beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, KFold\n",
    "X, X_test, y, y_test = sklearn.model_selection.train_test_split(X,y,test_size=0.3,random_state=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GradienDescent:\n",
      "The summarized loss for tau = 0.001 is 17.0\n",
      "The summarized loss for tau = 0.01 is 18.0\n",
      "The summarized loss for tau = 0.1 is 48.0\n",
      "SGD:\n",
      "The summarized loss for tau = 0.001 ,gamma= 0.0001 is 183.0\n",
      "The summarized loss for tau = 0.001 ,gamma= 0.001 is 183.0\n",
      "The summarized loss for tau = 0.001 ,gamma= 0.01 is 183.0\n",
      "The summarized loss for tau = 0.01 ,gamma= 0.0001 is 183.0\n",
      "The summarized loss for tau = 0.01 ,gamma= 0.001 is 183.0\n",
      "The summarized loss for tau = 0.01 ,gamma= 0.01 is 183.0\n",
      "The summarized loss for tau = 0.1 ,gamma= 0.0001 is 183.0\n",
      "The summarized loss for tau = 0.1 ,gamma= 0.001 is 183.0\n",
      "The summarized loss for tau = 0.1 ,gamma= 0.01 is 183.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split, KFold\n",
    "\n",
    "gamma_set = np.array([1e-4,1e-3,1e-2])\n",
    "mu_set = np.array([0.1,0.2,0.5])\n",
    "tau_set = np.array([1e-3,1e-2,1e-1])\n",
    "b0 = np.zeros(65)\n",
    "\n",
    "\n",
    "kf = sklearn.model_selection.KFold(n_splits = 10, shuffle = False, random_state = 0)\n",
    "print(\"GradienDescent:\")\n",
    "for tau in tau_set:\n",
    "    sum = 0\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "        beta = gradient_descent(10,b0,X_train,y_train,tau)\n",
    "        sum += zero_ones_loss(y_test, predict(-beta,X_test))\n",
    "    print(\"The summarized loss for tau =\",tau,\"is\",sum)\n",
    "\n",
    "print(\"SGD:\")\n",
    "for tau in tau_set:\n",
    "    for gamma in gamma_set:\n",
    "        sum = 0\n",
    "        count = 0\n",
    "        for train_index, test_index in kf.split(X):\n",
    "            count +=1\n",
    "            #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "            X_train, X_test = X[train_index], X[test_index]\n",
    "            y_train, y_test = y[train_index], y[test_index]\n",
    "        \n",
    "            beta = stochastic_gradient(150,b0,X_train,y_train,tau,gamma)\n",
    "            sum += zero_ones_loss(y_test, predict(beta,X_test))\n",
    "        print(\"The summarized loss for tau =\",tau,\",gamma=\",gamma,\"is\",sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class KFold in module sklearn.model_selection._split:\n",
      "\n",
      "class KFold(_BaseKFold)\n",
      " |  K-Folds cross-validator\n",
      " |  \n",
      " |  Provides train/test indices to split data in train/test sets. Split\n",
      " |  dataset into k consecutive folds (without shuffling by default).\n",
      " |  \n",
      " |  Each fold is then used once as a validation while the k - 1 remaining\n",
      " |  folds form the training set.\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <cross_validation>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  n_splits : int, default=3\n",
      " |      Number of folds. Must be at least 2.\n",
      " |  \n",
      " |      .. versionchanged:: 0.20\n",
      " |          ``n_splits`` default value will change from 3 to 5 in v0.22.\n",
      " |  \n",
      " |  shuffle : boolean, optional\n",
      " |      Whether to shuffle the data before splitting into batches.\n",
      " |  \n",
      " |  random_state : int, RandomState instance or None, optional, default=None\n",
      " |      If int, random_state is the seed used by the random number generator;\n",
      " |      If RandomState instance, random_state is the random number generator;\n",
      " |      If None, the random number generator is the RandomState instance used\n",
      " |      by `np.random`. Used when ``shuffle`` == True.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> from sklearn.model_selection import KFold\n",
      " |  >>> X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])\n",
      " |  >>> y = np.array([1, 2, 3, 4])\n",
      " |  >>> kf = KFold(n_splits=2)\n",
      " |  >>> kf.get_n_splits(X)\n",
      " |  2\n",
      " |  >>> print(kf)  # doctest: +NORMALIZE_WHITESPACE\n",
      " |  KFold(n_splits=2, random_state=None, shuffle=False)\n",
      " |  >>> for train_index, test_index in kf.split(X):\n",
      " |  ...    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
      " |  ...    X_train, X_test = X[train_index], X[test_index]\n",
      " |  ...    y_train, y_test = y[train_index], y[test_index]\n",
      " |  TRAIN: [2 3] TEST: [0 1]\n",
      " |  TRAIN: [0 1] TEST: [2 3]\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  The first ``n_samples % n_splits`` folds have size\n",
      " |  ``n_samples // n_splits + 1``, other folds have size\n",
      " |  ``n_samples // n_splits``, where ``n_samples`` is the number of samples.\n",
      " |  \n",
      " |  Randomized CV splitters may return different results for each call of\n",
      " |  split. You can make the results identical by setting ``random_state``\n",
      " |  to an integer.\n",
      " |  \n",
      " |  See also\n",
      " |  --------\n",
      " |  StratifiedKFold\n",
      " |      Takes group information into account to avoid building folds with\n",
      " |      imbalanced class distributions (for binary or multiclass\n",
      " |      classification tasks).\n",
      " |  \n",
      " |  GroupKFold: K-fold iterator variant with non-overlapping groups.\n",
      " |  \n",
      " |  RepeatedKFold: Repeats K-Fold n times.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      KFold\n",
      " |      _BaseKFold\n",
      " |      abc.NewBase\n",
      " |      BaseCrossValidator\n",
      " |      abc.NewBase\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, n_splits='warn', shuffle=False, random_state=None)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from _BaseKFold:\n",
      " |  \n",
      " |  get_n_splits(self, X=None, y=None, groups=None)\n",
      " |      Returns the number of splitting iterations in the cross-validator\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : object\n",
      " |          Always ignored, exists for compatibility.\n",
      " |      \n",
      " |      y : object\n",
      " |          Always ignored, exists for compatibility.\n",
      " |      \n",
      " |      groups : object\n",
      " |          Always ignored, exists for compatibility.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      n_splits : int\n",
      " |          Returns the number of splitting iterations in the cross-validator.\n",
      " |  \n",
      " |  split(self, X, y=None, groups=None)\n",
      " |      Generate indices to split data into training and test set.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape (n_samples, n_features)\n",
      " |          Training data, where n_samples is the number of samples\n",
      " |          and n_features is the number of features.\n",
      " |      \n",
      " |      y : array-like, shape (n_samples,)\n",
      " |          The target variable for supervised learning problems.\n",
      " |      \n",
      " |      groups : array-like, with shape (n_samples,), optional\n",
      " |          Group labels for the samples used while splitting the dataset into\n",
      " |          train/test set.\n",
      " |      \n",
      " |      Yields\n",
      " |      ------\n",
      " |      train : ndarray\n",
      " |          The training set indices for that split.\n",
      " |      \n",
      " |      test : ndarray\n",
      " |          The testing set indices for that split.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from BaseCrossValidator:\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from abc.NewBase:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(sklearn.model_selection.KFold)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
