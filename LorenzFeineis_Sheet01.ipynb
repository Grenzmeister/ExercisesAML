{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "\n",
    "digits = load_digits()\n",
    "\n",
    "data = digits[\"data\"]\n",
    "images = digits[\"images\"]\n",
    "target = digits[\"target\"]\n",
    "target_names = digits[\"target_names\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract necessary data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X is: (357, 65)\n",
      "Shape of y is: (357,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "for i in range(target.shape[0]):\n",
    "    if target[i]==3:\n",
    "        X.append(data[i])\n",
    "        y.append(1)\n",
    "    elif target[i]==8:\n",
    "        X.append(data[i])\n",
    "        y.append(-1)\n",
    "    else: continue\n",
    "\n",
    "y = np.array(y)\n",
    "X = np.array(X)\n",
    "ones = np.ones(X.shape[0],dtype=float)\n",
    "ones = ones[:, np.newaxis]\n",
    "X = np.concatenate((X, ones),axis=1)\n",
    "print(\"Shape of X is:\", X.shape)\n",
    "print(\"Shape of y is:\", y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression and Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best value for C is: [0.00413201]\n",
      "The score is: <bound method LogisticRegressionCV.score of LogisticRegressionCV(Cs=100, class_weight=None, cv=100, dual=False,\n",
      "           fit_intercept=False, intercept_scaling=1.0, max_iter=100,\n",
      "           multi_class='ovr', n_jobs=None, penalty='l2', random_state=0,\n",
      "           refit=True, scoring=None, solver='liblinear', tol=0.0001,\n",
      "           verbose=0)>\n",
      "The regularization parameter is: [242.01282648]\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "import sklearn as sk\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "\n",
    "clf = LogisticRegressionCV(cv = 100, Cs = 100, random_state=0, \n",
    "                        solver =\"liblinear\",\n",
    "                        fit_intercept = False,\n",
    "                        refit = True,\n",
    "                        multi_class=\"ovr\").fit(X,y)\n",
    "\n",
    "print(\"The best value for C is:\", clf.C_)\n",
    "print(\"The score is:\", clf.score)\n",
    "print(\"The regularization parameter is:\", 1/clf.C_)\n",
    "print(clf.Cs)\n",
    "reg_param = 1/clf.C_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.2 Optimization Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    sigma =  1/(1+np.exp(-z))\n",
    "    return sigma\n",
    "\n",
    "\n",
    "def gradient(beta, X, y):\n",
    "    update = np.zeros(beta.shape[0])\n",
    "    for i in range(X.shape[0]):\n",
    "        update = update + sigmoid(y[i]*np.dot(X[i,:],beta)) * y[i] * X[i,:]\n",
    "    grad = (1/reg_param) * beta + (1/y.shape[0]) * update\n",
    "    return grad\n",
    "\n",
    "def gradient_achita(beta, X, y):\n",
    "    N = y.shape[0]\n",
    "    temp = sigmoid(-y*np.dot(X,beta))*y\n",
    "    #Here is the correction of the grad\n",
    "    result = beta - 1.0/N*(np.dot(X.T,temp))\n",
    "    return result\n",
    "\n",
    "def predict(beta,X):\n",
    "    decision = np.dot(X,beta)\n",
    "    y_computed = np.sign(decision)\n",
    "    return y_computed\n",
    "\n",
    "def zero_ones_loss(y_prediction, y_truth):\n",
    "    zero_ones_vec = 1/2 * (y_prediction - y_truth)\n",
    "    return np.dot(zero_ones_vec , zero_ones_vec)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradien descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(m,beta,tau,X,y):\n",
    "    for i in range(m):\n",
    "        beta = beta - tau * gradient(beta,X,y)\n",
    "    return beta\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stochastic gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "183.0"
      ]
     },
     "execution_count": 350,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def stochastic_gradient(m,beta,X,y,tau0,gamma):\n",
    "    tau = tau0\n",
    "    for i in range(m):\n",
    "        tau = tau/(1+np.power(gamma,i))\n",
    "        X_random = X[np.random.randint(0,X.shape[0]),:].reshape(1,65)\n",
    "        beta = beta - tau * gradient(beta, X_random, y)\n",
    "    return beta\n",
    "\n",
    "zero_ones_loss(predict( stochastic_gradient(150,np.zeros(65),X,y,0.1,0.01) , X),y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SG minibatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loss is: 179.0\n"
     ]
    }
   ],
   "source": [
    "def SG_mini_batch(m,beta,X,y,tau0,gamma,B):\n",
    "    tau = tau0\n",
    "    for i in range(m):\n",
    "        tau = tau/(1+np.power(gamma,i))\n",
    "        rows_random = np.random.choice(np.arange(X.shape[0]),B,replace = True)\n",
    "        X_random = X[rows_random,:]\n",
    "        beta = beta - tau * gradient(beta,X_random,y)\n",
    "    return beta\n",
    "\n",
    "beta = SG_mini_batch(150,np.zeros(65),X,y,0.1,0.01,20)\n",
    "print(\"The loss is:\", zero_ones_loss(predict(beta,X),y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SG momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loss is: 183.0\n"
     ]
    }
   ],
   "source": [
    "def SG_momentum(m,beta,X,y,tau0,gamma,mu):\n",
    "    g = np.zeros(65)\n",
    "    tau = tau0\n",
    "    for i in range(m):\n",
    "        tau = tau/(1+np.power(gamma,i))\n",
    "        X_random = X[np.random.randint(0,X.shape[0]),:].reshape(1,65)\n",
    "        g = mu*g +(1-mu)*gradient(beta,X_random,y)\n",
    "        beta = beta - tau*g\n",
    "    return beta\n",
    "\n",
    "beta = SG_momentum(150,np.zeros(65),X,y,0.1,0.01,0.9)\n",
    "print(\"The loss is:\", zero_ones_loss(predict(beta,X),y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ADAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loss is: 183.0\n"
     ]
    }
   ],
   "source": [
    "def ADAM(m,beta,X,y,tau):\n",
    "    g = 0\n",
    "    q = 1\n",
    "    mu1 = 0.9\n",
    "    mu2 = 0.999\n",
    "    epsilon = 1e-8\n",
    "    for i in range(m):\n",
    "        X_random = X[np.random.randint(0,X.shape[0]),:].reshape(1,65)\n",
    "        grad = gradient(beta,X_random,y)     \n",
    "        g = mu1*g + (1-mu1)*grad\n",
    "        q = mu2*q + (1-mu2)*(grad*grad)\n",
    "        g_tilt = g/(1-np.power(mu1,i+1))\n",
    "        q_tilt = q/(1-np.power(mu2,i+1))  \n",
    "        beta = beta - tau/(np.sqrt(q_tilt)+epsilon) * g_tilt\n",
    "    return beta\n",
    "\n",
    "\n",
    "\n",
    "beta = ADAM(150,np.zeros(65),X,y,0.1)\n",
    "print(\"The loss is:\", zero_ones_loss(predict(beta,X),y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "stochastic average gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loss is: 20.0\n"
     ]
    }
   ],
   "source": [
    "def SAG(m,beta,X,y,tau0,gamma):\n",
    "    g_stored = []\n",
    "    tau = tau0\n",
    "    for i in range(y.shape[0]):\n",
    "        g_stored.append( y[i]* X[i] * sigmoid(-y[i] * np.dot( X[i] , beta)))\n",
    "    g_stored = np.array(g_stored)\n",
    "    g = np.sum(g_stored,axis = 0)/y.shape[0]\n",
    "    for j in range(m):\n",
    "        tau = tau/(1+np.power(gamma,j))\n",
    "        i = np.random.randint(0,g_stored.shape[0])\n",
    "        g_update = y[i] * np.dot( X[i] , sigmoid(-y[i]* np.dot(X[i] , beta)))\n",
    "        g = g + 1/X.shape[0] *(g_update - g_stored[i])\n",
    "        g_stored[i] = g_update\n",
    "        beta = beta * (1- tau/reg_param) - tau*g \n",
    "    return beta\n",
    "\n",
    "beta = SAG(150,np.zeros(65),X,y,0.1,0.01)\n",
    "print(\"The loss is:\", zero_ones_loss(predict(-beta,X),y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dual coordinate ascent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loss is: 70.0\n"
     ]
    }
   ],
   "source": [
    "def DCA(m):\n",
    "    alpha = np.random.rand(y.shape[0])    \n",
    "    beta = reg_param/y.shape[0] * np.dot( alpha * y, X)\n",
    "    for j in range(m):\n",
    "        i = np.random.randint(alpha.shape[0])\n",
    "        f1 = y[i]*np.dot(X[i],beta) + np.log(alpha[i]/(1-alpha[i]))      \n",
    "        f2 = reg_param/y.shape[0] * np.dot(X[i],X[i]) + 1/(alpha[i]*(1-alpha[i]))       \n",
    "        alpha_new = alpha[i] - f1/f2\n",
    "        beta = beta + (reg_param/y.shape[0]) *y[i]* X[i] * (alpha_new - alpha[i])\n",
    "    return beta\n",
    "\n",
    "beta = DCA(10)\n",
    "print(\"The loss is:\", zero_ones_loss(predict(beta,X),y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loss is: 0.0\n"
     ]
    }
   ],
   "source": [
    "def newton_raphson(m):\n",
    "    for t in range(m):\n",
    "        D = X.shape[1]\n",
    "        N = X.shape[0]\n",
    "        beta = np.zeros(D)\n",
    "        z = np.dot( X , beta)\n",
    "        y_tilt = y/sigmoid(np.dot(y,z))\n",
    "        diag = np.array([reg_param/N*sigmoid(z[i])*sigmoid(-z[i]) for i in range(N)])\n",
    "        W = np.diagflat(diag)\n",
    "        beta = np.dot( np.dot( np.linalg.inv( np.identity(D) + np.dot ( np.dot(X.T , W) , X )) , np.dot(X.T,W) ), z + y_tilt)\n",
    "    return beta\n",
    "\n",
    "beta = newton_raphson(10)\n",
    "print(\"The loss is:\", zero_ones_loss(predict(beta,X),y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 2, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 3, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 4, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 5, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 6, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 7, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 8, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 9]])"
      ]
     },
     "execution_count": 340,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L = [i for i in range(10)]\n",
    "np.diagflat(L)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
