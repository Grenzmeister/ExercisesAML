{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sheet01\n",
    "## Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "\n",
    "digits = load_digits()\n",
    "\n",
    "data = digits[\"data\"]\n",
    "images = digits[\"images\"]\n",
    "target = digits[\"target\"]\n",
    "target_names = digits[\"target_names\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract necessary data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X is: (357, 65)\n",
      "Shape of y is: (357,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "for i in range(target.shape[0]):\n",
    "    if target[i]==3:\n",
    "        X.append(data[i])\n",
    "        y.append(1)\n",
    "    elif target[i]==8:\n",
    "        X.append(data[i])\n",
    "        y.append(-1)\n",
    "    else: continue\n",
    "\n",
    "y = np.array(y)\n",
    "X = np.array(X)\n",
    "ones = np.ones(X.shape[0],dtype=float)\n",
    "ones = ones[:, np.newaxis]\n",
    "X = np.concatenate((X, ones),axis=1)\n",
    "print(\"Shape of X is:\", X.shape)\n",
    "print(\"Shape of y is:\", y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression and Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best value for C is: [0.00413201]\n",
      "The score is: <bound method LogisticRegressionCV.score of LogisticRegressionCV(Cs=100, class_weight=None, cv=100, dual=False,\n",
      "           fit_intercept=False, intercept_scaling=1.0, max_iter=100,\n",
      "           multi_class='ovr', n_jobs=None, penalty='l2', random_state=0,\n",
      "           refit=True, scoring=None, solver='liblinear', tol=0.0001,\n",
      "           verbose=0)>\n",
      "The regularization parameter is: [242.01282648]\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "import sklearn as sk\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "\n",
    "clf = LogisticRegressionCV(cv = 100, Cs = 100, random_state=0, \n",
    "                        solver =\"liblinear\",\n",
    "                        fit_intercept = False,\n",
    "                        refit = True,\n",
    "                        multi_class=\"ovr\").fit(X,y)\n",
    "\n",
    "print(\"The best value for C is:\", clf.C_)\n",
    "print(\"The score is:\", clf.score)\n",
    "print(\"The regularization parameter is:\", 1/clf.C_)\n",
    "print(clf.Cs)\n",
    "reg_param = 1/clf.C_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Optimization Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    sigma =  1/(1+np.exp(-z))\n",
    "    return sigma\n",
    "\n",
    "\n",
    "def gradient(beta, X, y):\n",
    "    update = np.zeros(beta.shape[0])\n",
    "    for i in range(X.shape[0]):\n",
    "        update = update + sigmoid(y[i]*np.dot(X[i,:],beta)) * y[i] * X[i,:]\n",
    "    grad = (1/reg_param) * beta + (1/y.shape[0]) * update\n",
    "    return grad\n",
    "\n",
    "def gradient_achita(beta, X, y):\n",
    "    N = y.shape[0]\n",
    "    temp = sigmoid(-y*np.dot(X,beta))*y\n",
    "    #Here is the correction of the grad\n",
    "    result = beta - 1.0/N*(np.dot(X.T,temp))\n",
    "    return result\n",
    "\n",
    "def predict(beta,X):\n",
    "    decision = np.dot(X,beta)\n",
    "    y_computed = np.sign(decision)\n",
    "    return y_computed\n",
    "\n",
    "def zero_ones_loss(y_prediction, y_truth):\n",
    "    zero_ones_vec = 1/2 * (y_prediction - y_truth)\n",
    "    return np.dot(zero_ones_vec , zero_ones_vec)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradien descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(m,beta,X,y,tau):\n",
    "    for i in range(m):\n",
    "        beta = beta - tau * gradient(beta,X,y)\n",
    "    return beta\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "183.0"
      ]
     },
     "execution_count": 455,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def stochastic_gradient(m,beta,X,y,tau0,gamma):\n",
    "    tau = tau0\n",
    "    for i in range(m):\n",
    "        tau = tau/(1+np.power(gamma,i))\n",
    "        X_random = X[np.random.randint(0,X.shape[0]),:].reshape(1,65)\n",
    "        beta = beta - tau * gradient(beta, X_random, y)\n",
    "    return beta\n",
    "\n",
    "zero_ones_loss(predict( stochastic_gradient(150,np.zeros(65),X,y,0.1,0.01) , X),y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SG minibatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loss is: 179.0\n"
     ]
    }
   ],
   "source": [
    "def SG_mini_batch(m,beta,X,y,tau0,gamma,B):\n",
    "    tau = tau0\n",
    "    for i in range(m):\n",
    "        tau = tau/(1+np.power(gamma,i))\n",
    "        rows_random = np.random.choice(np.arange(X.shape[0]),B,replace = True)\n",
    "        X_random = X[rows_random,:]\n",
    "        beta = beta - tau * gradient(beta,X_random,y)\n",
    "    return beta\n",
    "\n",
    "beta = SG_mini_batch(150,np.zeros(65),X,y,0.1,0.01,20)\n",
    "print(\"The loss is:\", zero_ones_loss(predict(beta,X),y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SG momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loss is: 183.0\n"
     ]
    }
   ],
   "source": [
    "def SG_momentum(m,beta,X,y,tau0,gamma,mu):\n",
    "    g = np.zeros(65)\n",
    "    tau = tau0\n",
    "    for i in range(m):\n",
    "        tau = tau/(1+np.power(gamma,i))\n",
    "        X_random = X[np.random.randint(0,X.shape[0]),:].reshape(1,65)\n",
    "        g = mu*g +(1-mu)*gradient(beta,X_random,y)\n",
    "        beta = beta - tau*g\n",
    "    return beta\n",
    "\n",
    "beta = SG_momentum(150,np.zeros(65),X,y,0.1,0.01,0.9)\n",
    "print(\"The loss is:\", zero_ones_loss(predict(beta,X),y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ADAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loss is: 183.0\n"
     ]
    }
   ],
   "source": [
    "def ADAM(m,beta,X,y,tau):\n",
    "    g = 0\n",
    "    q = 1\n",
    "    mu1 = 0.9\n",
    "    mu2 = 0.999\n",
    "    epsilon = 1e-8\n",
    "    for i in range(m):\n",
    "        X_random = X[np.random.randint(0,X.shape[0]),:].reshape(1,65)\n",
    "        grad = gradient(beta,X_random,y)     \n",
    "        g = mu1*g + (1-mu1)*grad\n",
    "        q = mu2*q + (1-mu2)*(grad*grad)\n",
    "        g_tilt = g/(1-np.power(mu1,i+1))\n",
    "        q_tilt = q/(1-np.power(mu2,i+1))  \n",
    "        beta = beta - tau/(np.sqrt(q_tilt)+epsilon) * g_tilt\n",
    "    return beta\n",
    "\n",
    "\n",
    "\n",
    "beta = ADAM(150,np.zeros(65),X,y,0.1)\n",
    "print(\"The loss is:\", zero_ones_loss(predict(beta,X),y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### stochastic average gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loss is: 20.0\n"
     ]
    }
   ],
   "source": [
    "def SAG(m,beta,X,y,tau0,gamma):\n",
    "    g_stored = []\n",
    "    tau = tau0\n",
    "    for i in range(y.shape[0]):\n",
    "        g_stored.append( y[i]* X[i] * sigmoid(-y[i] * np.dot( X[i] , beta)))\n",
    "    g_stored = np.array(g_stored)\n",
    "    g = np.sum(g_stored,axis = 0)/y.shape[0]\n",
    "    for j in range(m):\n",
    "        tau = tau/(1+np.power(gamma,j))\n",
    "        i = np.random.randint(0,g_stored.shape[0])\n",
    "        g_update = y[i] * np.dot( X[i] , sigmoid(-y[i]* np.dot(X[i] , beta)))\n",
    "        g = g + 1/X.shape[0] *(g_update - g_stored[i])\n",
    "        g_stored[i] = g_update\n",
    "        beta = beta * (1- tau/reg_param) - tau*g \n",
    "    return beta\n",
    "\n",
    "beta = SAG(150,np.zeros(65),X,y,0.1,0.01)\n",
    "print(\"The loss is:\", zero_ones_loss(predict(-beta,X),y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dual coordinate ascent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loss is: 16.0\n"
     ]
    }
   ],
   "source": [
    "def DCA(m,X,y):\n",
    "    alpha = np.random.rand(y.shape[0])    \n",
    "    beta = reg_param/y.shape[0] * np.dot( alpha * y, X)\n",
    "    for j in range(m):\n",
    "        i = np.random.randint(alpha.shape[0])\n",
    "        f1 = y[i]*np.dot(X[i],beta) + np.log(alpha[i]/(1-alpha[i]))      \n",
    "        f2 = reg_param/y.shape[0] * np.dot(X[i],X[i]) + 1/(alpha[i]*(1-alpha[i]))       \n",
    "        alpha_new = alpha[i] - f1/f2\n",
    "        beta = beta + (reg_param/y.shape[0]) *y[i]* X[i] * (alpha_new - alpha[i])\n",
    "    return beta\n",
    "\n",
    "beta = DCA(10,X,y)\n",
    "print(\"The loss is:\", zero_ones_loss(predict(beta,X),y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### newton/raphson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loss is: 0.0\n"
     ]
    }
   ],
   "source": [
    "def newton_raphson(m,beta,X,y):\n",
    "    for t in range(m):\n",
    "        D = X.shape[1]\n",
    "        N = X.shape[0]\n",
    "        beta = np.zeros(D)\n",
    "        z = np.dot( X , beta)\n",
    "        y_tilt = y/sigmoid(np.dot(y,z))\n",
    "        diag = np.array([reg_param/N*sigmoid(z[i])*sigmoid(-z[i]) for i in range(N)])\n",
    "        W = np.diagflat(diag)\n",
    "        beta = np.dot( np.dot( np.linalg.inv( np.identity(D) + np.dot ( np.dot(X.T , W) , X )) , np.dot(X.T,W) ), z + y_tilt)\n",
    "    return beta\n",
    "\n",
    "beta = newton_raphson(10,np.zeros(65),X,y)\n",
    "print(\"The loss is:\", zero_ones_loss(predict(beta,X),y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, KFold\n",
    "X, X_test, y, y_test = sklearn.model_selection.train_test_split(X,y,test_size=0.3,random_state=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GradienDescent:\n",
      "The summarized loss for tau = 0.001 is 17.0\n",
      "The summarized loss for tau = 0.01 is 18.0\n",
      "The summarized loss for tau = 0.1 is 48.0\n",
      "SGD:\n",
      "The summarized loss for tau = 0.001 ,gamma= 0.0001 is 183.0\n",
      "The summarized loss for tau = 0.001 ,gamma= 0.001 is 183.0\n",
      "The summarized loss for tau = 0.001 ,gamma= 0.01 is 183.0\n",
      "The summarized loss for tau = 0.01 ,gamma= 0.0001 is 183.0\n",
      "The summarized loss for tau = 0.01 ,gamma= 0.001 is 183.0\n",
      "The summarized loss for tau = 0.01 ,gamma= 0.01 is 183.0\n",
      "The summarized loss for tau = 0.1 ,gamma= 0.0001 is 183.0\n",
      "The summarized loss for tau = 0.1 ,gamma= 0.001 is 183.0\n",
      "The summarized loss for tau = 0.1 ,gamma= 0.01 is 183.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split, KFold\n",
    "\n",
    "gamma_set = np.array([1e-4,1e-3,1e-2])\n",
    "mu_set = np.array([0.1,0.2,0.5])\n",
    "tau_set = np.array([1e-3,1e-2,1e-1])\n",
    "b0 = np.zeros(65)\n",
    "\n",
    "\n",
    "kf = sklearn.model_selection.KFold(n_splits = 10, shuffle = False, random_state = 0)\n",
    "print(\"GradienDescent:\")\n",
    "for tau in tau_set:\n",
    "    sum = 0\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "        beta = gradient_descent(10,b0,X_train,y_train,tau)\n",
    "        sum += zero_ones_loss(y_test, predict(-beta,X_test))\n",
    "    print(\"The summarized loss for tau =\",tau,\"is\",sum)\n",
    "\n",
    "print(\"SGD:\")\n",
    "for tau in tau_set:\n",
    "    for gamma in gamma_set:\n",
    "        sum = 0\n",
    "        count = 0\n",
    "        for train_index, test_index in kf.split(X):\n",
    "            count +=1\n",
    "            #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "            X_train, X_test = X[train_index], X[test_index]\n",
    "            y_train, y_test = y[train_index], y[test_index]\n",
    "        \n",
    "            beta = stochastic_gradient(150,b0,X_train,y_train,tau,gamma)\n",
    "            sum += zero_ones_loss(y_test, predict(beta,X_test))\n",
    "        print(\"The summarized loss for tau =\",tau,\",gamma=\",gamma,\"is\",sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class KFold in module sklearn.model_selection._split:\n",
      "\n",
      "class KFold(_BaseKFold)\n",
      " |  K-Folds cross-validator\n",
      " |  \n",
      " |  Provides train/test indices to split data in train/test sets. Split\n",
      " |  dataset into k consecutive folds (without shuffling by default).\n",
      " |  \n",
      " |  Each fold is then used once as a validation while the k - 1 remaining\n",
      " |  folds form the training set.\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <cross_validation>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  n_splits : int, default=3\n",
      " |      Number of folds. Must be at least 2.\n",
      " |  \n",
      " |      .. versionchanged:: 0.20\n",
      " |          ``n_splits`` default value will change from 3 to 5 in v0.22.\n",
      " |  \n",
      " |  shuffle : boolean, optional\n",
      " |      Whether to shuffle the data before splitting into batches.\n",
      " |  \n",
      " |  random_state : int, RandomState instance or None, optional, default=None\n",
      " |      If int, random_state is the seed used by the random number generator;\n",
      " |      If RandomState instance, random_state is the random number generator;\n",
      " |      If None, the random number generator is the RandomState instance used\n",
      " |      by `np.random`. Used when ``shuffle`` == True.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> from sklearn.model_selection import KFold\n",
      " |  >>> X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])\n",
      " |  >>> y = np.array([1, 2, 3, 4])\n",
      " |  >>> kf = KFold(n_splits=2)\n",
      " |  >>> kf.get_n_splits(X)\n",
      " |  2\n",
      " |  >>> print(kf)  # doctest: +NORMALIZE_WHITESPACE\n",
      " |  KFold(n_splits=2, random_state=None, shuffle=False)\n",
      " |  >>> for train_index, test_index in kf.split(X):\n",
      " |  ...    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
      " |  ...    X_train, X_test = X[train_index], X[test_index]\n",
      " |  ...    y_train, y_test = y[train_index], y[test_index]\n",
      " |  TRAIN: [2 3] TEST: [0 1]\n",
      " |  TRAIN: [0 1] TEST: [2 3]\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  The first ``n_samples % n_splits`` folds have size\n",
      " |  ``n_samples // n_splits + 1``, other folds have size\n",
      " |  ``n_samples // n_splits``, where ``n_samples`` is the number of samples.\n",
      " |  \n",
      " |  Randomized CV splitters may return different results for each call of\n",
      " |  split. You can make the results identical by setting ``random_state``\n",
      " |  to an integer.\n",
      " |  \n",
      " |  See also\n",
      " |  --------\n",
      " |  StratifiedKFold\n",
      " |      Takes group information into account to avoid building folds with\n",
      " |      imbalanced class distributions (for binary or multiclass\n",
      " |      classification tasks).\n",
      " |  \n",
      " |  GroupKFold: K-fold iterator variant with non-overlapping groups.\n",
      " |  \n",
      " |  RepeatedKFold: Repeats K-Fold n times.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      KFold\n",
      " |      _BaseKFold\n",
      " |      abc.NewBase\n",
      " |      BaseCrossValidator\n",
      " |      abc.NewBase\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, n_splits='warn', shuffle=False, random_state=None)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from _BaseKFold:\n",
      " |  \n",
      " |  get_n_splits(self, X=None, y=None, groups=None)\n",
      " |      Returns the number of splitting iterations in the cross-validator\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : object\n",
      " |          Always ignored, exists for compatibility.\n",
      " |      \n",
      " |      y : object\n",
      " |          Always ignored, exists for compatibility.\n",
      " |      \n",
      " |      groups : object\n",
      " |          Always ignored, exists for compatibility.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      n_splits : int\n",
      " |          Returns the number of splitting iterations in the cross-validator.\n",
      " |  \n",
      " |  split(self, X, y=None, groups=None)\n",
      " |      Generate indices to split data into training and test set.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape (n_samples, n_features)\n",
      " |          Training data, where n_samples is the number of samples\n",
      " |          and n_features is the number of features.\n",
      " |      \n",
      " |      y : array-like, shape (n_samples,)\n",
      " |          The target variable for supervised learning problems.\n",
      " |      \n",
      " |      groups : array-like, with shape (n_samples,), optional\n",
      " |          Group labels for the samples used while splitting the dataset into\n",
      " |          train/test set.\n",
      " |      \n",
      " |      Yields\n",
      " |      ------\n",
      " |      train : ndarray\n",
      " |          The training set indices for that split.\n",
      " |      \n",
      " |      test : ndarray\n",
      " |          The testing set indices for that split.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from BaseCrossValidator:\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from abc.NewBase:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(sklearn.model_selection.KFold)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
