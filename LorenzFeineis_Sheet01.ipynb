{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sheet01\n",
    "## Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "\n",
    "digits = load_digits()\n",
    "\n",
    "data = digits[\"data\"]\n",
    "images = digits[\"images\"]\n",
    "target = digits[\"target\"]\n",
    "target_names = digits[\"target_names\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract necessary data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X is: (357, 65)\n",
      "Shape of y is: (357,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "for i in range(target.shape[0]):\n",
    "    if target[i]==3:\n",
    "        X.append(data[i])\n",
    "        y.append(1)\n",
    "    elif target[i]==8:\n",
    "        X.append(data[i])\n",
    "        y.append(-1)\n",
    "    else: continue\n",
    "\n",
    "y = np.array(y)\n",
    "X = np.array(X)\n",
    "ones = np.ones(X.shape[0],dtype=float)\n",
    "ones = ones[:, np.newaxis]\n",
    "X = np.concatenate((X, ones),axis=1)\n",
    "print(\"Shape of X is:\", X.shape)\n",
    "print(\"Shape of y is:\", y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression and Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The maximum score 1.0 is at lamb = 231.0129700083158\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.3, random_state=0)\n",
    "\n",
    "scores = []\n",
    "C_list = np.geomspace(1e-4,1e2,100)\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "for lamb_inv in C_list:\n",
    "    clf1 = LogisticRegression(C = lamb_inv, fit_intercept = False, random_state = 0, solver = \"liblinear\").fit(X_train,y_train)\n",
    "    plt.hold = True\n",
    "    ax.plot(lamb_inv, clf1.score(X_test,y_test),\"ro\")\n",
    "    scores.append(clf1.score(X_test,y_test))\n",
    "\n",
    "scores = np.array(scores)\n",
    "lamb = 1/C_list[np.argmax(scores)]\n",
    "reg_param = lamb\n",
    "plt.show()\n",
    "print(\"The maximum score\",np.max(scores),\"is at lamb =\",lamb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Optimization Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    sigma =  1/(1+np.exp(-z))\n",
    "    return sigma\n",
    "\n",
    "\n",
    "def gradient(beta, X, y):\n",
    "    update = np.zeros(beta.shape[0])\n",
    "    for i in range(X.shape[0]):\n",
    "        update = update + sigmoid(y[i]*np.dot(X[i,:],beta)) * y[i] * X[i,:]\n",
    "    grad = (1/reg_param) * beta + (1/y.shape[0]) * update\n",
    "    return grad\n",
    "\n",
    "def gradient_achita(beta, X, y):\n",
    "    N = y.shape[0]\n",
    "    temp = sigmoid(-y*np.dot(X,beta))*y\n",
    "    #Here is the correction of the grad\n",
    "    result = beta - 1.0/N*(np.dot(X.T,temp))\n",
    "    return result\n",
    "\n",
    "def predict(beta,X):\n",
    "    decision = np.dot(X,beta)\n",
    "    y_computed = np.sign(decision)\n",
    "    return y_computed\n",
    "\n",
    "def zero_ones_loss(y_prediction, y_truth):\n",
    "    zero_ones_vec = 1/2 * (y_prediction - y_truth)\n",
    "    return np.dot(zero_ones_vec , zero_ones_vec)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(m,beta,X,y,tau):\n",
    "    for i in range(m):\n",
    "        beta = beta - tau * gradient(beta,X,y)\n",
    "    return beta\n",
    "\n",
    "def stochastic_gradient(m,beta,X,y,tau0,gamma):\n",
    "    tau = tau0\n",
    "    for i in range(m):\n",
    "        tau = tau/(1+np.power(gamma,i))\n",
    "        X_random = X[np.random.randint(0,X.shape[0]),:].reshape(1,65)\n",
    "        beta = beta - tau * gradient(beta, X_random, y)\n",
    "    return beta\n",
    "\n",
    "def SG_mini_batch(m,beta,X,y,tau0,gamma,B):\n",
    "    tau = tau0\n",
    "    for i in range(m):\n",
    "        tau = tau/(1+np.power(gamma,i))\n",
    "        rows_random = np.random.choice(np.arange(X.shape[0]),B,replace = True)\n",
    "        X_random = X[rows_random,:]\n",
    "        beta = beta - tau * gradient(beta,X_random,y)\n",
    "    return beta\n",
    "\n",
    "def SG_momentum(m,beta,X,y,tau0,gamma,mu):\n",
    "    g = np.zeros(65)\n",
    "    tau = tau0\n",
    "    for i in range(m):\n",
    "        tau = tau/(1+np.power(gamma,i))\n",
    "        X_random = X[np.random.randint(0,X.shape[0]),:].reshape(1,65)\n",
    "        g = mu*g +(1-mu)*gradient(beta,X_random,y)\n",
    "        beta = beta - tau*g\n",
    "    return beta\n",
    "\n",
    "def ADAM(m,beta,X,y,tau):\n",
    "    g = 0\n",
    "    q = 1\n",
    "    mu1 = 0.9\n",
    "    mu2 = 0.999\n",
    "    epsilon = 1e-8\n",
    "    for i in range(m):\n",
    "        X_random = X[np.random.randint(0,X.shape[0]),:].reshape(1,65)\n",
    "        grad = gradient(beta,X_random,y)     \n",
    "        g = mu1*g + (1-mu1)*grad\n",
    "        q = mu2*q + (1-mu2)*(grad*grad)\n",
    "        g_tilt = g/(1-np.power(mu1,i+1))\n",
    "        q_tilt = q/(1-np.power(mu2,i+1))  \n",
    "        beta = beta - tau/(np.sqrt(q_tilt)+epsilon) * g_tilt\n",
    "    return beta\n",
    "\n",
    "def SAG(m,beta,X,y,tau0,gamma):\n",
    "    g_stored = []\n",
    "    tau = tau0\n",
    "    for i in range(y.shape[0]):\n",
    "        g_stored.append( y[i]* X[i] * sigmoid(-y[i] * np.dot( X[i] , beta)))\n",
    "    g_stored = np.array(g_stored)\n",
    "    g = np.sum(g_stored,axis = 0)/y.shape[0]\n",
    "    for j in range(m):\n",
    "        tau = tau/(1+np.power(gamma,j))\n",
    "        i = np.random.randint(0,g_stored.shape[0])\n",
    "        g_update = y[i] * np.dot( X[i] , sigmoid(-y[i]* np.dot(X[i] , beta)))\n",
    "        g = g + 1/X.shape[0] *(g_update - g_stored[i])\n",
    "        g_stored[i] = g_update\n",
    "        beta = beta * (1- tau/reg_param) - tau*g \n",
    "    return beta\n",
    "\n",
    "\n",
    "def DCA(m,X,y):\n",
    "    alpha = np.random.rand(y.shape[0])    \n",
    "    beta = reg_param/y.shape[0] * np.dot( alpha * y, X)\n",
    "    for j in range(m):\n",
    "        i = np.random.randint(alpha.shape[0])\n",
    "        f1 = y[i]*np.dot(X[i],beta) + np.log(alpha[i]/(1-alpha[i]))      \n",
    "        f2 = reg_param/y.shape[0] * np.dot(X[i],X[i]) + 1/(alpha[i]*(1-alpha[i]))       \n",
    "        alpha_new = alpha[i] - f1/f2\n",
    "        beta = beta + (reg_param/y.shape[0]) *y[i]* X[i] * (alpha_new - alpha[i])\n",
    "    return beta\n",
    "\n",
    "\n",
    "def newton_raphson(m,beta,X,y):\n",
    "    for t in range(m):\n",
    "        D = X.shape[1]\n",
    "        N = X.shape[0]\n",
    "        beta = np.zeros(D)\n",
    "        z = np.dot( X , beta)\n",
    "        y_tilt = y/sigmoid(np.dot(y,z))\n",
    "        diag = np.array([reg_param/N*sigmoid(z[i])*sigmoid(-z[i]) for i in range(N)])\n",
    "        W = np.diagflat(diag)\n",
    "        beta = np.dot( np.dot( np.linalg.inv( np.identity(D) + np.dot ( np.dot(X.T , W) , X )) , np.dot(X.T,W) ), z + y_tilt)\n",
    "    return beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GD,tau0 107.0\n",
      "SG 57.0\n",
      "SG_mini_ 52.0\n",
      "SG_momentum 57.0\n",
      "ADAM 57.0\n",
      "SAG 72.0\n",
      "DCA 46.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lorenz/.local/lib/python3.5/site-packages/ipykernel_launcher.py:2: RuntimeWarning: overflow encountered in exp\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Newton/Raphson 0.0\n"
     ]
    }
   ],
   "source": [
    "N = y_train.shape[0]\n",
    "D = X.shape[1]\n",
    "beta0 = np.zeros(D)\n",
    "g0 = 0\n",
    "alpha0 = np.random.uniform(size = N)\n",
    "tau0 = 0.1\n",
    "mu = 0.1\n",
    "gamma = 0.01\n",
    "B = 20\n",
    "print(\"GD,tau0\", zero_ones_loss(predict( gradient_descent(150, beta0 ,X_train ,y_train ,tau0) ,X_test),y_test))\n",
    "print(\"SG\" , zero_ones_loss(predict( stochastic_gradient(150, beta0 ,X_train ,y_train ,tau0,gamma) ,X_test),y_test))\n",
    "print(\"SG_mini_\", zero_ones_loss(predict( SG_mini_batch(150, beta0 ,X_train ,y_train ,tau0,gamma,B) ,X_test),y_test))\n",
    "print(\"SG_momentum\",zero_ones_loss(predict( SG_momentum(150, beta0 ,X_train ,y_train ,tau0,gamma,mu) ,X_test),y_test))\n",
    "print(\"ADAM\", zero_ones_loss(predict( ADAM(150, beta0 ,X_train ,y_train ,tau0) ,X_test),y_test))\n",
    "print(\"SAG\", zero_ones_loss(predict( SAG(150, beta0 ,X_train ,y_train ,tau0,gamma) ,X_test),y_test))\n",
    "\n",
    "print(\"DCA\", zero_ones_loss(predict( DCA(150,X_train ,y_train) ,X_test),y_test))\n",
    "print(\"Newton/Raphson\",zero_ones_loss(predict( newton_raphson(150,beta0,X_train ,y_train) ,X_test),y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding the best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best tau for Gradient Descent\n",
      "For tau= 0.001 the summarized Loss is 340.0\n",
      "For tau= 0.01 the summarized Loss is 339.0\n",
      "For tau= 0.1 the summarized Loss is 309.0\n",
      "Best tau and gamma for stochastic gradient\n",
      "For tau= 0.001 gamma= 0.0001 the summarized Loss is 183.0\n",
      "For tau= 0.001 gamma= 0.001 the summarized Loss is 183.0\n",
      "For tau= 0.001 gamma= 0.01 the summarized Loss is 183.0\n",
      "For tau= 0.01 gamma= 0.0001 the summarized Loss is 183.0\n",
      "For tau= 0.01 gamma= 0.001 the summarized Loss is 183.0\n",
      "For tau= 0.01 gamma= 0.01 the summarized Loss is 183.0\n",
      "For tau= 0.1 gamma= 0.0001 the summarized Loss is 183.0\n",
      "For tau= 0.1 gamma= 0.001 the summarized Loss is 183.0\n",
      "For tau= 0.1 gamma= 0.01 the summarized Loss is 183.0\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'GD_ADAM' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-98-9d6c171b623b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0msum_ADAM\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mzero_ones_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mADAM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbeta0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtau0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0mGD_HP\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtau_index\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum_GD\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m     \u001b[0mGD_ADAM\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtau_index\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum_ADAM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mgamma_index\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mgamma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGAMMA\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgamma_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'GD_ADAM' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kf = KFold(n_splits = 10)\n",
    "\n",
    "TAU = np.array([1e-3,1e-2,0.1])\n",
    "MU = np.array([0.1,0.2,0.5])\n",
    "GAMMA = np.array([1e-4,1e-3,1e-2])\n",
    "\n",
    "print(\"Best tau for Gradient Descent\")\n",
    "for tau0 in TAU:\n",
    "    sum = 0\n",
    "    for train_index, test_index in kf.split(X,y):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        sum = sum + zero_ones_loss(predict(gradient_descent(10,beta0,X_train,y_train,tau0),X_test),y_test)\n",
    "    print(\"For tau=\",tau0,\"the summarized Loss is\",sum)\n",
    "    \n",
    "print(\"Best tau and gamma for stochastic gradient\")\n",
    "for tau0 in TAU:\n",
    "    for gamma in GAMMA:\n",
    "        sum = 0\n",
    "        for train_index, test_index in kf.split(X,y):\n",
    "            X_train, X_test = X[train_index], X[test_index]\n",
    "            y_train, y_test = y[train_index], y[test_index]\n",
    "            sum = sum + zero_ones_loss(predict(stochastic_gradient(150,beta0,X_train,y_train,tau0,gamma),X_test),y_test)\n",
    "        print(\"For tau=\",tau0,\"gamma=\",gamma,\"the summarized Loss is\",sum)\n",
    "        \n",
    "GD_HP = np.array([TAU,[0,0,0]])\n",
    "SGD_HP = np.array([[TAU],[GAMMA],[np.zeros(9)]])\n",
    "SGMBatch_HP = np.array([[TAU],[GAMMA],[0,0,0]])\n",
    "SGMomentum_HP = np.array([[[TAU],[GAMMA],[MU],[np.zeros(27)]]])\n",
    "ADAM_HP = np.array([TAU,[0,0,0]])\n",
    "SAG_HP = np.array([[TAU],[GAMMA],[np.zeros(9)]])\n",
    "\n",
    "for tau_index in range(3):\n",
    "    tau0 = TAU[tau_index]\n",
    "    sum_GD = 0\n",
    "    sum_ADAM = 0\n",
    "    for train_index, test_index in kf.split(X,y):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        sum_GD += zero_ones_loss(predict(gradient_descent(10,beta0,X_train,y_train,tau0),X_test),y_test)\n",
    "        sum_ADAM += zero_ones_loss(predict(ADAM(10,beta0,X_train,y_train,tau0),X_test),y_test)\n",
    "    GD_HP[1,tau_index] = sum_GD\n",
    "    ADAM_HP[1,tau_index] = sum_ADAM\n",
    "    for gamma_index in range(3):\n",
    "        gamma = GAMMA[gamma_index]\n",
    "        for mu_index in range(3):\n",
    "            mu = MU[mu_index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
