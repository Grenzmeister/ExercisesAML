{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sheet01\n",
    "## Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "\n",
    "digits = load_digits()\n",
    "\n",
    "data = digits[\"data\"]\n",
    "images = digits[\"images\"]\n",
    "target = digits[\"target\"]\n",
    "target_names = digits[\"target_names\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract necessary data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X is: (357, 65)\n",
      "Shape of y is: (357,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "for i in range(target.shape[0]):\n",
    "    if target[i]==3:\n",
    "        X.append(data[i])\n",
    "        y.append(1)\n",
    "    elif target[i]==8:\n",
    "        X.append(data[i])\n",
    "        y.append(-1)\n",
    "    else: continue\n",
    "\n",
    "y = np.array(y)\n",
    "X = np.array(X)\n",
    "ones = np.ones(X.shape[0],dtype=float)\n",
    "ones = ones[:, np.newaxis]\n",
    "X = np.concatenate((X, ones),axis=1)\n",
    "print(\"Shape of X is:\", X.shape)\n",
    "print(\"Shape of y is:\", y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression and Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The maximum score 1.0 is at lamb = 231.0129700083158\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.3, random_state=0)\n",
    "\n",
    "scores = []\n",
    "C_list = np.geomspace(1e-4,1e2,100)\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "for lamb_inv in C_list:\n",
    "    clf1 = LogisticRegression(C = lamb_inv, fit_intercept = False, random_state = 0, solver = \"liblinear\").fit(X_train,y_train)\n",
    "    plt.hold = True\n",
    "    ax.plot(lamb_inv, clf1.score(X_test,y_test),\"ro\")\n",
    "    scores.append(clf1.score(X_test,y_test))\n",
    "\n",
    "scores = np.array(scores)\n",
    "lamb = 1/C_list[np.argmax(scores)]\n",
    "reg_param = lamb\n",
    "plt.show()\n",
    "print(\"The maximum score\",np.max(scores),\"is at lamb =\",lamb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Optimization Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    sigma =  1/(1+np.exp(-z))\n",
    "    return sigma\n",
    "\n",
    "\n",
    "def gradient(beta, X, y):\n",
    "    update = np.zeros(beta.shape[0])\n",
    "    for i in range(X.shape[0]):\n",
    "        update = update + sigmoid(y[i]*np.dot(X[i,:],beta)) * y[i] * X[i,:]\n",
    "    grad = (1/reg_param) * beta + (1/y.shape[0]) * update\n",
    "    return grad\n",
    "\n",
    "def gradient_achita(beta, X, y):\n",
    "    N = y.shape[0]\n",
    "    temp = sigmoid(-y*np.dot(X,beta))*y\n",
    "    #Here is the correction of the grad\n",
    "    result = beta - 1.0/N*(np.dot(X.T,temp))\n",
    "    return result\n",
    "\n",
    "def predict(beta,X):\n",
    "    decision = np.dot(X,beta)\n",
    "    y_computed = np.sign(decision)\n",
    "    return y_computed\n",
    "\n",
    "def zero_ones_loss(y_prediction, y_truth):\n",
    "    zero_ones_vec = 1/2 * (y_prediction - y_truth)\n",
    "    return np.dot(zero_ones_vec , zero_ones_vec)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(m,beta,X,y,tau):\n",
    "    for i in range(m):\n",
    "        beta = beta - tau * gradient(beta,X,y)\n",
    "    return beta\n",
    "\n",
    "def stochastic_gradient(m,beta,X,y,tau0,gamma):\n",
    "    tau = tau0\n",
    "    for i in range(m):\n",
    "        tau = tau/(1+np.power(gamma,i))\n",
    "        X_random = X[np.random.randint(0,X.shape[0]),:].reshape(1,65)\n",
    "        beta = beta - tau * gradient(beta, X_random, y)\n",
    "    return beta\n",
    "\n",
    "def SG_mini_batch(m,beta,X,y,tau0,gamma,B):\n",
    "    tau = tau0\n",
    "    for i in range(m):\n",
    "        tau = tau/(1+gamma*i)\n",
    "        rows_random = np.random.choice(np.arange(X.shape[0]),B,replace = True)\n",
    "        X_random = X[rows_random,:]\n",
    "        beta = beta - tau * gradient(beta,X_random,y)\n",
    "    return beta\n",
    "\n",
    "def SG_momentum(m,beta,X,y,tau0,gamma,mu):\n",
    "    g = np.zeros(65)\n",
    "    tau = tau0\n",
    "    for i in range(m):\n",
    "        tau = tau/(1+gamma*i)\n",
    "        X_random = X[np.random.randint(0,X.shape[0]),:].reshape(1,65)\n",
    "        g = mu*g +(1-mu)*gradient(beta,X_random,y)\n",
    "        beta = beta - tau*g\n",
    "    return beta\n",
    "\n",
    "def ADAM(m,beta,X,y,tau):\n",
    "    g = 0\n",
    "    q = 1\n",
    "    mu1 = 0.9\n",
    "    mu2 = 0.999\n",
    "    epsilon = 1e-8\n",
    "    for i in range(m):\n",
    "        X_random = X[np.random.randint(0,X.shape[0]),:].reshape(1,65)\n",
    "        grad = gradient(beta,X_random,y)     \n",
    "        g = mu1*g + (1-mu1)*grad\n",
    "        q = mu2*q + (1-mu2)*(grad*grad)\n",
    "        g_tilt = g/(1-np.power(mu1,i+1))\n",
    "        q_tilt = q/(1-np.power(mu2,i+1))  \n",
    "        beta = beta - tau/(np.sqrt(q_tilt)+epsilon) * g_tilt\n",
    "    return beta\n",
    "\n",
    "def SAG(m,beta,X,y,tau0,gamma):\n",
    "    g_stored = []\n",
    "    tau = tau0\n",
    "    for i in range(y.shape[0]):\n",
    "        g_stored.append( y[i]* X[i] * sigmoid(-y[i] * np.dot( X[i] , beta)))\n",
    "    g_stored = np.array(g_stored)\n",
    "    g = np.sum(g_stored,axis = 0)/y.shape[0]\n",
    "    for j in range(m):\n",
    "        tau = tau/(1+gamma*i)\n",
    "        i = np.random.randint(0,g_stored.shape[0])\n",
    "        g_update = y[i] * np.dot( X[i] , sigmoid(-y[i]* np.dot(X[i] , beta)))\n",
    "        g = g + 1/X.shape[0] *(g_update - g_stored[i])\n",
    "        g_stored[i] = g_update\n",
    "        beta = beta * (1- tau/reg_param) - tau*g \n",
    "    return beta\n",
    "\n",
    "\n",
    "def DCA(m,X,y):\n",
    "    alpha = np.random.rand(y.shape[0])    \n",
    "    beta = reg_param/y.shape[0] * np.dot( alpha * y, X)\n",
    "    for j in range(m):\n",
    "        i = np.random.randint(alpha.shape[0])\n",
    "        f1 = y[i]*np.dot(X[i],beta) + np.log(alpha[i]/(1-alpha[i]))      \n",
    "        f2 = reg_param/y.shape[0] * np.dot(X[i],X[i]) + 1/(alpha[i]*(1-alpha[i]))       \n",
    "        alpha_new = alpha[i] - f1/f2\n",
    "        beta = beta + (reg_param/y.shape[0]) *y[i]* X[i] * (alpha_new - alpha[i])\n",
    "    return beta\n",
    "\n",
    "\n",
    "def newton_raphson(m,beta,X,y):\n",
    "    for t in range(m):\n",
    "        D = X.shape[1]\n",
    "        N = X.shape[0]\n",
    "        beta = np.zeros(D)\n",
    "        z = np.dot( X , beta)\n",
    "        y_tilt = y/sigmoid(np.dot(y,z))\n",
    "        diag = np.array([reg_param/N*sigmoid(z[i])*sigmoid(-z[i]) for i in range(N)])\n",
    "        W = np.diagflat(diag)\n",
    "        beta = np.dot( np.dot( np.linalg.inv( np.identity(D) + np.dot ( np.dot(X.T , W) , X )) , np.dot(X.T,W) ), z + y_tilt)\n",
    "    return beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GD,tau0 107.0\n",
      "SG 57.0\n",
      "SG_mini_ 55.0\n",
      "SG_momentum 57.0\n",
      "ADAM 57.0\n",
      "SAG 106.0\n",
      "DCA 55.0\n",
      "Newton/Raphson 0.0\n"
     ]
    }
   ],
   "source": [
    "N = y_train.shape[0]\n",
    "D = X.shape[1]\n",
    "beta0 = np.zeros(D)\n",
    "g0 = 0\n",
    "alpha0 = np.random.uniform(size = N)\n",
    "tau0 = 0.1\n",
    "mu = 0.1\n",
    "gamma = 0.01\n",
    "B = 20\n",
    "print(\"GD,tau0\", zero_ones_loss(predict( gradient_descent(150, beta0 ,X_train ,y_train ,tau0) ,X_test),y_test))\n",
    "print(\"SG\" , zero_ones_loss(predict( stochastic_gradient(150, beta0 ,X_train ,y_train ,tau0,gamma) ,X_test),y_test))\n",
    "print(\"SG_mini_\", zero_ones_loss(predict( SG_mini_batch(150, beta0 ,X_train ,y_train ,tau0,gamma,B) ,X_test),y_test))\n",
    "print(\"SG_momentum\",zero_ones_loss(predict( SG_momentum(150, beta0 ,X_train ,y_train ,tau0,gamma,mu) ,X_test),y_test))\n",
    "print(\"ADAM\", zero_ones_loss(predict( ADAM(150, beta0 ,X_train ,y_train ,tau0) ,X_test),y_test))\n",
    "print(\"SAG\", zero_ones_loss(predict( SAG(150, beta0 ,X_train ,y_train ,tau0,gamma) ,X_test),y_test))\n",
    "\n",
    "print(\"DCA\", zero_ones_loss(predict( DCA(150,X_train ,y_train) ,X_test),y_test))\n",
    "print(\"Newton/Raphson\",zero_ones_loss(predict( newton_raphson(150,beta0,X_train ,y_train) ,X_test),y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding the best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lorenz/.local/lib/python3.5/site-packages/ipykernel_launcher.py:2: RuntimeWarning: overflow encountered in exp\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kf = KFold(n_splits = 10)\n",
    "\n",
    "TAU = np.array([1e-3,1e-2,0.1])\n",
    "MU = np.array([0.1,0.2,0.5])\n",
    "GAMMA = np.array([1e-4,1e-3,1e-2])\n",
    "\n",
    "        \n",
    "GD_HP = np.array([TAU,[0,0,0]])\n",
    "SGD_HP = np.zeros((3,3))\n",
    "SGMBatch_HP = np.zeros((3,3))\n",
    "SGMomentum_HP = np.zeros((3,3,3))\n",
    "ADAM_HP = np.array([TAU,[0,0,0]])\n",
    "SAG_HP = np.zeros((3,3))\n",
    "\n",
    "for tau_index in range(3):\n",
    "    tau0 = TAU[tau_index]\n",
    "    sum_GD = 0\n",
    "    sum_ADAM = 0\n",
    "    for train_index, test_index in kf.split(X,y):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        sum_GD += zero_ones_loss(predict(gradient_descent(10,beta0,X_train,y_train,tau0),X_test),y_test)\n",
    "        sum_ADAM += zero_ones_loss(predict(ADAM(10,beta0,X_train,y_train,tau0),X_test),y_test)\n",
    "    GD_HP[1,tau_index] = sum_GD\n",
    "    ADAM_HP[1,tau_index] = sum_ADAM\n",
    "    for gamma_index in range(3):\n",
    "        gamma = GAMMA[gamma_index]\n",
    "        sum_SG = 0\n",
    "        sum_SGMBatch = 0\n",
    "        sum_SAG = 0\n",
    "        for train_index, test_index in kf.split(X,y):\n",
    "            X_train, X_test = X[train_index], X[test_index]\n",
    "            y_train, y_test = y[train_index], y[test_index]\n",
    "            sum_SG +=  zero_ones_loss(predict(stochastic_gradient(150,beta0,X_train,y_train,tau0,gamma),X_test),y_test)\n",
    "            sum_SGMBatch += zero_ones_loss(predict(SG_mini_batch(150,beta0,X_train,y_train,tau0,gamma,20),X_test),y_test)\n",
    "            sum_SAG += zero_ones_loss(predict(SAG(150,beta0,X_train,y_train,tau0,gamma),X_test),y_test)\n",
    "        SGD_HP[tau_index,gamma_index] = sum_SG\n",
    "        SGMBatch_HP[tau_index,gamma_index] = sum_SGMBatch\n",
    "        SAG_HP[tau_index,gamma_index] = sum_SAG\n",
    "        for mu_index in range(3):\n",
    "            mu = MU[mu_index]\n",
    "            sum_SGMomentum = 0\n",
    "            for train_index, test_index in kf.split(X,y):\n",
    "                X_train, X_test = X[train_index], X[test_index]\n",
    "                y_train, y_test = y[train_index], y[test_index]\n",
    "                sum_SGMomentum += zero_ones_loss(predict(SG_momentum(150,beta0,X_train,y_train,tau0,gamma,mu),X_test),y_test)\n",
    "            SGMomentum_HP[tau_index,gamma_index,mu_index] = sum_SGMomentum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For gradient descent the best learning rate is 0.1\n",
      "For ADAM the best learning rate is 0.001\n",
      "[[183. 183. 183.]\n",
      " [183. 183. 183.]\n",
      " [183. 183. 183.]]\n",
      "For stochastic GD the best learning rate is 0.001\n",
      "For stochastic mini batch the best learning rate is 0.1\n",
      "and the best gamma is 0.001\n",
      "For SAG the best learning rate is 0.1\n",
      "%%!and the best gamma is 0.001\n"
     ]
    }
   ],
   "source": [
    "print(\"For gradient descent the best learning rate is\", GD_HP[0,np.argmin(GD_HP[1,:])])\n",
    "print(\"For ADAM the best learning rate is\",ADAM_HP[0,np.argmin(ADAM_HP[1,:])])\n",
    "print(SGD_HP)\n",
    "print(\"For stochastic GD the best learning rate is\", TAU[np.argmin(SGD_HP)//3])\n",
    "print(\"For stochastic mini batch the best learning rate is\", TAU[np.argmin(SGMBatch_HP)//3])\n",
    "print(\"and the best gamma is\", GAMMA[np.argmin(SGMBatch_HP)%3])\n",
    "print(\"For SAG the best learning rate is\", TAU[np.argmin(SAG_HP)//3])\n",
    "print(\"and the best gamma is\", TAU[np.argmin(SAG_HP)%3])\n",
    "print(\"For SG Momentum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function argmin in module numpy.core.fromnumeric:\n",
      "\n",
      "argmin(a, axis=None, out=None)\n",
      "    Returns the indices of the minimum values along an axis.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    a : array_like\n",
      "        Input array.\n",
      "    axis : int, optional\n",
      "        By default, the index is into the flattened array, otherwise\n",
      "        along the specified axis.\n",
      "    out : array, optional\n",
      "        If provided, the result will be inserted into this array. It should\n",
      "        be of the appropriate shape and dtype.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    index_array : ndarray of ints\n",
      "        Array of indices into the array. It has the same shape as `a.shape`\n",
      "        with the dimension along `axis` removed.\n",
      "    \n",
      "    See Also\n",
      "    --------\n",
      "    ndarray.argmin, argmax\n",
      "    amin : The minimum value along a given axis.\n",
      "    unravel_index : Convert a flat index into an index tuple.\n",
      "    \n",
      "    Notes\n",
      "    -----\n",
      "    In case of multiple occurrences of the minimum values, the indices\n",
      "    corresponding to the first occurrence are returned.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> a = np.arange(6).reshape(2,3)\n",
      "    >>> a\n",
      "    array([[0, 1, 2],\n",
      "           [3, 4, 5]])\n",
      "    >>> np.argmin(a)\n",
      "    0\n",
      "    >>> np.argmin(a, axis=0)\n",
      "    array([0, 0, 0])\n",
      "    >>> np.argmin(a, axis=1)\n",
      "    array([0, 0])\n",
      "    \n",
      "    Indices of the minimum elements of a N-dimensional array:\n",
      "    \n",
      "    >>> ind = np.unravel_index(np.argmin(a, axis=None), a.shape)\n",
      "    >>> ind\n",
      "    (0, 0)\n",
      "    >>> a[ind]\n",
      "    0\n",
      "    \n",
      "    >>> b = np.arange(6)\n",
      "    >>> b[4] = 0\n",
      "    >>> b\n",
      "    array([0, 1, 2, 3, 0, 5])\n",
      "    >>> np.argmin(b)  # Only the first occurrence is returned.\n",
      "    0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(np.argmin)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
